{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#welcome-to-the-trac-training-on","title":"Welcome to the TrAC Training on","text":""},{"location":"#demystifying-trending-ai-techniques","title":"Demystifying Trending AI Techniques","text":"<p>Hi all! welcome to TrAC training on Demystifying Trending AI Techniques.   Date: October 20th, 2023  Time: 1 PM \u2013 5 PM  Venue: Online (zoom)</p>"},{"location":"#before-you-get-started-please-read-our","title":"Before you get started, please read our:","text":"<ol> <li> <p> Code of Conduct</p> </li> <li> <p> Logistics</p> </li> <li> <p> Schedule</p> </li> <li> <p> Tutorial Slides </p> </li> <li> <p> External Resources</p> </li> </ol>"},{"location":"cloud/codespaces/","title":"GitHub CodeSpaces","text":"<p>CodeSpaces are virtual services which provides \"cloud-based development environment\" for software programmers and data scientists. </p> <p>CodeSpaces is run on cloud services (Microsoft Azure), and links with your GitHub account for a seamless experience working on code in a Git repository.</p>"},{"location":"cloud/codespaces/#starting-codespace","title":"Starting CodeSpace","text":"<p>When a GitHub Organization and Repository have CodeSpaces enabled you will see a \"Code\" button above the README.md</p> <p></p> <p>Click on the \"Code\" button and start a new CodeSpace</p> <p></p> <p>Select the size of the CodeSpace you want (2-4 cores and 4GB to 8GB of RAM should be plenty for today)</p> <p></p> <p>Click \"Create CodeSpace\"</p> <p>You will be taken to a loading screen, and after a few moments (&lt;2 minutes) your browser will change to a VS Code instance in your browser.</p> <p></p> <p>Notice, the GitHub repository where you initiated the CodeSpace is set as the working directory of the EXPLORER  in the upper left side of VS Code interface. You're in your Git repo, and are able to work with Python, Docker, Node, or any one of many featured developer tools. Further, you can install any tools you like!</p> <p></p>"},{"location":"cloud/js2/","title":"Jetstream-2","text":"<p>To log into the JupyterHub, go to: <code>http://tractrainXX.cyverse.org/</code> -- you will be assigned a node designated by <code>XX</code>, e.g. <code>01</code>, <code>02</code>, ..., <code>15</code> </p> <p>Use your GitHub credentials to log into the Hub.</p> <p>A Jupyter Lab should open for you.</p>"},{"location":"cloud/js2/#get-access-to-jetstream-2-for-yourself-or-your-research-group","title":"Get Access to Jetstream-2 for yourself or your research group","text":"<p>Jetstream-2 is a public research cloud funded by the National Science Foundation. </p> <p>Access to Jetstream-2 is managed by XSEDE.org</p> <p>To get access to Jetstream-2 you must:</p> <ul> <li> <p>Create a XSEDE Portal Account</p> </li> <li> <p>Request a Start-up Allocation under \"Allocations\" &gt; \"Submit/Review Request\"</p> </li> <li> <p>For Jetstream-2 write a short project description and justification (typically less than 3 pages) </p> </li> </ul> <p>After you have been approved, you will be able to access Jetstream-2 using either their web-based Exosphere interface, </p> <p>or you can explore using other command-line API options from their documentation. </p>"},{"location":"getting_started/code_conduct/","title":"Code of Conduct","text":"<p>All attendees, speakers, staff, and volunteers are required to follow our code of conduct.</p> <p>Iowa State University expects and appreciates cooperation from all participants to help ensure a safe, collaborative environment for everyone. Harrassment by any individual will not be tolerated and may result in the individual being removed from the Camp.</p> <p>Harassment includes: offensive verbal comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion, technology choices, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention.</p> <p>Participants who are asked to stop any harassing behavior are expected to comply immediately.</p> <p>Workshop staff are also subject to the anti-harassment policy. In particular, staff should not use sexualised images, activities, or other material.</p> <p>If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund.</p> <p>If you are being harassed, or notice that someone else is being harassed, or have any other concerns, please contact a member of the workshop staff immediately. Staff can be identified as they\u2019ll be wearing badges or nametags.</p> <p>Workshop staff will be happy to help participants contact local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance.</p> <p>We expect participants to follow these rules at conference and workshop venues and conference-related social events.</p> <p>See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.</p>"},{"location":"getting_started/logistics/","title":"Logistics","text":"<p>Date: October 20th, 2023  Time: 1 PM \u2013 5 PM  Venue: Zoom</p>"},{"location":"getting_started/schedule/","title":"Schedule","text":"<p>All times shown in Central Daylight Time</p> <p>Date: October 20th, 2023</p> <p>Time: 1 PM \u2013 5 PM</p> <p>Venue: Hoover 2055</p> Time Concept 01:00p - 1:15p Introduction 01:15 - 02:00 Basics of AI 02:00 - 02:40 AI till 2023 at a glance 02:45 - 03:00 Break 03:00 - 04:00 Generative AI and its implications 04:00 - 04:45 Trends in AI 04:45 - 05:00 Final concluding remarks and whats next?"},{"location":"sections/m01_intro/","title":"Introductory Exercise","text":"<p>Generate an image of a cow without mentioning the word \"cow\"</p> <ul> <li>Come up with a description of a cow grazing on a field </li> <li>Then use the following tools to generate an image using that description:<ul> <li>https://beta.dreamstudio.ai/generate</li> <li>https://huggingface.co/spaces/stabilityai/stable-diffusion</li> <li>https://www.bing.com/create</li> </ul> </li> </ul>"},{"location":"sections/m03_multimod/","title":"Whisper","text":"<ul> <li>Whisper is an automatic speech recognition (ASR) system</li> <li>Trained on 680,000 hours of multilingual and multitask supervised data collected from the web</li> </ul> <p>Check out:</p> <ul> <li>https://openai.com/research/whisper</li> <li>https://huggingface.co/spaces/sanchit-gandhi/whisper-jax</li> </ul>"},{"location":"sections/m04_novelalgo/","title":"Insect Detection App (ISU)","text":"<ul> <li>https://insectapp.las.iastate.edu/</li> </ul>"},{"location":"sections/m04_novelalgo/#deep-q-learning-example","title":"Deep Q-Learning Example","text":"<ul> <li>DQN paper</li> <li>Google DeepMind's Deep Q-learning playing Atari Breakout!</li> </ul>"},{"location":"sections/resources/","title":"Resources","text":"<ol> <li>Introduction<ol> <li>Tools for generating images using description<ol> <li>https://beta.dreamstudio.ai/generate</li> <li>https://huggingface.co/spaces/stabilityai/stable-diffusion</li> <li>https://www.bing.com/create</li> </ol> </li> <li>Research paper collection<ol> <li>https://aman.ai/papers/</li> </ol> </li> <li>Improving deep learning performance<ol> <li>https://horace.io/brrr_intro.html</li> </ol> </li> <li>Multi-model deep learning<ol> <li>https://arxiv.org/pdf/2301.04856.pdf</li> </ol> </li> <li>AI-based ppt generation<ol> <li>https://slidesgpt.com/index.html</li> </ol> </li> <li>Visualize your dreams<ol> <li>https://beta.dreamstudio.ai/generate</li> </ol> </li> <li>ChatGPT<ol> <li>https://chat.openai.com/auth/login</li> </ol> </li> </ol> </li> <li>Multiple modalities of Data to enhance AI<ol> <li>Computer Vision<ol> <li> <ul> <li>Additional models for classification, pose estimation, depth estimation, and ReID are just gotten from paperswithcode rankings</li> </ul> </li> <li>YOLOv8 is very good for exploring different computer vision tasks since it has detection, segmentation, classification, pose, and tracking (ByteTrack and BoT-SORT without ReID for now) implemented</li> <li>Object detection<ol> <li>CNN-based<ol> <li>Requires relatively little data (especially if pretrained with datasets such as COCO) fast inference</li> <li>YOLOv8 (still no actual paper)<ul> <li>https://github.com/ultralytics/ultralytics </li> <li>https://blog.roboflow.com/whats-new-in-yolov8/ </li> </ul> </li> <li>YOLOv6.3<ul> <li>Situationally better than v8, but it isn't as user-friendly and doesn't have additional functionality (segmentation, classification, pose)</li> <li>https://arxiv.org/abs/2301.05586 </li> </ul> </li> </ol> </li> <li>Transformer-based detection<ol> <li>Generally requires more data and has slower inference but can detect many more types of objects/classes</li> <li>DETR<ol> <li>https://github.com/facebookresearch/detr </li> <li>https://arxiv.org/abs/2005.12872 </li> </ol> </li> <li>Grounding DINO: SOTA Zero-Shot Objection Detection<ol> <li>Can detect new classes with no additional training, one of their examples is a dog's tail</li> <li>https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/</li> <li>https://arxiv.org/abs/2303.05499 </li> </ol> </li> <li>DINO v2<ol> <li>https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/ </li> <li>https://arxiv.org/abs/2304.07193</li> </ol> </li> </ol> </li> </ol> </li> <li>Object segmentation<ol> <li>YOLOv8 also has segmentation<ol> <li>Segment anything</li> <li>https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/ </li> <li>https://segment-anything.com/</li> <li>https://huggingface.co/spaces/curt-park/segment-anything-with-clip</li> </ol> </li> </ol> </li> <li>Classification<ol> <li>YOLOv8 also has classification</li> <li>CoCa: Contrastive Captioners<ol> <li>https://arxiv.org/abs/2205.01917</li> </ol> </li> </ol> </li> <li>Pose estimation<ol> <li>YOLOv8 also has pose</li> <li>ViTPose+<ol> <li>https://github.com/vitae-transformer/vitpose </li> <li>https://arxiv.org/abs/2212.04246 </li> <li>https://arxiv.org/abs/2204.12484 </li> </ol> </li> </ol> </li> <li>Tracking<ol> <li>BoT-SORT<ol> <li>https://arxiv.org/abs/2206.14651 </li> </ol> </li> <li>Deep OC-SORT<ol> <li>Better than BoT-SORT in some metrics, and worse in others, but it also runs faster (good for edge devices where the lower fps of BoT-SORT can impact the motion model)</li> <li>https://arxiv.org/abs/2302.11813</li> </ol> </li> <li>Person ReID</li> <li>Fast-ReID: A Pytorch Toolbox for General Instance Re-identification<ol> <li>https://arxiv.org/abs/2006.02631</li> </ol> </li> <li>DenseIL \u2013 Dense Interaction Learning for Video-based Person Re-identification\u00a0<ol> <li>https://arxiv.org/pdf/2103.09013v3.pdf</li> </ol> </li> </ol> </li> <li>Depth estimation<ol> <li>HiMODE<ol> <li>https://arxiv.org/pdf/2204.05007v1.pdf</li> </ol> </li> <li>FreDSNet<ol> <li>https://github.com/sbrunoberenguel/fredsnet</li> <li>https://arxiv.org/pdf/2210.01595v1.pdf</li> </ol> </li> </ol> </li> </ol> </li> <li>Speech<ol> <li>Whisper<ol> <li>https://huggingface.co/spaces/sanchit-gandhi/whisper-jax</li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Novel Algorithms for enhancing AI</p> <ol> <li>Self-supervised learning<ol> <li>https://arxiv.org/pdf/2304.12210.pdf</li> </ol> </li> <li>Neural Fields<ol> <li>https://sites.google.com/berkeley.edu/nerf-tutorial</li> <li>https://arxiv.org/pdf/2111.11426.pdf</li> <li>https://arxiv.org/pdf/2210.00379.pdf</li> <li>https://neuralfields.cs.brown.edu/cvpr22.html</li> </ol> </li> </ol> </li> <li> <p>Generative AI and its implications</p> <ol> <li>Large Language Models<ol> <li>https://cims.nyu.edu/~sbowman/eightthings.pdf</li> <li>https://arxiv.org/pdf/2303.12712.pdf</li> <li>https://sebastianraschka.com/blog/2023/llm-reading-list.html?s=31</li> </ol> </li> <li>Transformers<ol> <li>https://arxiv.org/abs/2302.07730</li> <li>https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM</li> </ol> </li> </ol> </li> <li>Science in AI and AI in Science<ol> <li>Healthcare application<ol> <li>https://huggingface.co/spaces/suppfunterpno/Heart-Failure-Death-Prediction</li> </ol> </li> <li>Protein folding<ol> <li>https://esmatlas.com/resources/fold/result?fasta_header=%3EHallucinated%20protein%20HALC1_878&amp;sequence=MSGMKKLYEYTVTTLDEFLEKLKEFILNTSKDKIYKLTITNPKLIKDIGKAIAKAAEIADVDPKEIEEMIKAVEENELTKLVITIEQTDDKYVIKVELENEDGLVHSFEIYFKNKEEMEKFLELLEKLISKLSGS</li> </ol> </li> <li>Physics informed neural networks<ol> <li>https://docs.nvidia.com/deeplearning/modulus/modulus-sym/user_guide/theory/architectures.html#deeponet</li> <li>https://arxiv.org/pdf/2304.00567.pdf</li> <li>https://arxiv.org/pdf/2207.05748.pdf</li> <li>https://arxiv.org/pdf/2111.13587.pdf</li> <li>https://arxiv.org/pdf/2304.13799.pdf</li> <li>https://developer.nvidia.com/modulus?ncid=so-link-410285-vt25#cid=hpc03_so-link_en-us</li> </ol> </li> </ol> </li> <li>Trends in AI<ol> <li>https://www.capgemini.com/wp-content/uploads/2017/09/five_senses_pov.pdf</li> <li>https://aiindex.stanford.edu/wp-content/uploads/2021/11/2021-AI-Index-Report_Master.pdf</li> <li>https://hai.stanford.edu/sites/default/files/ai_index_2019_report.pdf</li> <li>https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report_2023.pdf</li> </ol> </li> </ol>"}]}